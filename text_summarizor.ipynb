{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 17 06:51:05 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.40       Driver Version: 516.40       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   63C    P0     9W /  N/A |      0MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "accelerate                0.27.2\n",
      "aiohttp                   3.9.3\n",
      "aiosignal                 1.3.1\n",
      "alembic                   1.13.1\n",
      "annotated-types           0.6.0\n",
      "anyio                     4.2.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "banal                     1.0.6\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "blis                      0.7.11\n",
      "catalogue                 2.0.10\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "cli_helpers               2.3.1\n",
      "click                     8.1.7\n",
      "cloudpathlib              0.16.0\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.1\n",
      "confection                0.1.4\n",
      "configobj                 5.0.8\n",
      "cymem                     2.0.8\n",
      "dataset                   1.6.2\n",
      "datasets                  2.17.1\n",
      "debugpy                   1.8.0\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.8\n",
      "emoji                     2.10.1\n",
      "en-core-web-sm            3.7.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.13.1\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2023.10.0\n",
      "gensim                    4.3.2\n",
      "greenlet                  3.0.3\n",
      "gunzip                    0.1.10\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.2\n",
      "httpx                     0.26.0\n",
      "huggingface-hub           0.20.3\n",
      "idna                      3.6\n",
      "ipykernel                 6.29.2\n",
      "ipython                   8.21.0\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.14\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.0\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.1\n",
      "jupyter-events            0.9.0\n",
      "jupyter-lsp               2.2.2\n",
      "jupyter_server            2.12.5\n",
      "jupyter_server_terminals  0.5.2\n",
      "jupyterlab                4.1.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.2\n",
      "jupyterlab_widgets        3.0.10\n",
      "langcodes                 3.3.0\n",
      "Mako                      1.3.2\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.6\n",
      "microapp                  0.3.15\n",
      "mistune                   3.0.2\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "multiprocess              0.70.16\n",
      "murmurhash                1.0.10\n",
      "nbclient                  0.9.0\n",
      "nbconvert                 7.16.0\n",
      "nbformat                  5.9.2\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.2.1\n",
      "nltk                      3.8.1\n",
      "notebook                  7.0.7\n",
      "notebook_shim             0.2.3\n",
      "numpy                     1.26.4\n",
      "overrides                 7.7.0\n",
      "packaging                 23.2\n",
      "pandas                    2.2.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pendulum                  3.0.0\n",
      "pgcli                     4.0.1\n",
      "pgspecial                 2.1.1\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "plotly                    5.19.0\n",
      "preshed                   3.0.9\n",
      "prometheus-client         0.19.0\n",
      "prompt-toolkit            3.0.43\n",
      "protobuf                  5.26.0\n",
      "psutil                    5.9.8\n",
      "psycopg                   3.1.18\n",
      "psycopg-binary            3.1.18\n",
      "psycopg-pool              3.2.1\n",
      "psycopg2-binary           2.9.9\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.0\n",
      "pyarrow-hotfix            0.6\n",
      "pycparser                 2.21\n",
      "pydantic                  2.6.1\n",
      "pydantic_core             2.16.2\n",
      "Pygments                  2.17.2\n",
      "python-dateutil           2.8.2\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.12\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "qtconsole                 5.5.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.33.0\n",
      "regex                     2023.12.25\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.17.1\n",
      "safetensors               0.4.2\n",
      "scikit-learn              1.4.0\n",
      "scipy                     1.12.0\n",
      "Send2Trash                1.8.2\n",
      "sentencepiece             0.2.0\n",
      "seqeval                   1.2.2\n",
      "setproctitle              1.3.3\n",
      "setuptools                65.5.0\n",
      "six                       1.16.0\n",
      "smart-open                6.4.0\n",
      "sniffio                   1.3.0\n",
      "soupsieve                 2.5\n",
      "spacey                    0.1.1\n",
      "spacy                     3.7.2\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.5\n",
      "SQLAlchemy                1.4.51\n",
      "sqlparse                  0.4.4\n",
      "srsly                     2.4.8\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tabulate                  0.9.0\n",
      "tenacity                  8.2.3\n",
      "terminado                 0.18.0\n",
      "textblob                  0.17.1\n",
      "thinc                     8.2.3\n",
      "threadpoolctl             3.2.0\n",
      "time-machine              2.13.0\n",
      "tinycss2                  1.2.1\n",
      "tokenizer                 3.4.3\n",
      "tokenizers                0.15.2\n",
      "torch                     2.2.1\n",
      "torcha                    0.1\n",
      "torchaudio                2.2.1+cpu\n",
      "torchvision               0.17.1\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.2\n",
      "traitlets                 5.14.1\n",
      "transformers              4.38.1\n",
      "typer                     0.9.0\n",
      "types-python-dateutil     2.8.19.20240106\n",
      "typing_extensions         4.9.0\n",
      "tzdata                    2023.4\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.0\n",
      "wasabi                    1.1.2\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.3.4\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wget                      3.2\n",
      "widgetsnbextension        4.0.10\n",
      "xxhash                    3.4.1\n",
      "yarl                      1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[sentencepiece]) (4.66.2)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting protobuf (from transformers[sentencepiece])\n",
      "  Downloading protobuf-5.26.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------  983.0/991.5 kB 31.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 991.5/991.5 kB 15.8 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.26.0-cp310-abi3-win_amd64.whl (420 kB)\n",
      "   ---------------------------------------- 0.0/420.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 420.9/420.9 kB 25.7 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, protobuf\n",
      "Successfully installed protobuf-5.26.0 sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]\n",
    "# The sentencepiece library is used for tokenization, which is the process of breaking down text into smaller units (tokens) that can be processed by the transformer models.\n",
    "# The sentencepiece library is particularly useful when working with transformer models that use a custom tokenizer, such as the ones used in BERT, RoBERTa, and other models.\n",
    "# These models often use a subword tokenization approach, where words are broken down into smaller subword units, which can improve the model's ability to handle out-of-vocabulary words and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataset in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataset) (1.4.51)\n",
      "Requirement already satisfied: alembic>=0.6.2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataset) (1.13.1)\n",
      "Requirement already satisfied: banal>=1.0.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataset) (1.0.6)\n",
      "Requirement already satisfied: Mako in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic>=0.6.2->dataset) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic>=0.6.2->dataset) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacrebleu) (5.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (4.66.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from portalocker->sacrebleu) (306)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu rouge_score\n",
    "# BLEU (Bilingual Evaluation Understudy) is a metric that compares the generated text to one or more reference translations, and it provides a score that reflects the similarity between the generated text and the reference translations.\n",
    "# sacrebleu simplifies the process of computing BLEU scores and ensures that the evaluation is done in a consistent and reproducible manner.\n",
    "# ROUGE metrics compare the generated summary to one or more reference summaries, and they provide scores that reflect the overlap between the generated summary and the reference summaries.\n",
    "# The rouge_score library includes several variants of the ROUGE metric, such as ROUGE-1, ROUGE-2, and ROUGE-L, which can be used to assess different aspects of the summary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: texttable in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (5.9.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install py7zr\n",
    "# unzip using 7zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score  py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Found existing installation: transformers 4.38.2\n",
      "Uninstalling transformers-4.38.2:\n",
      "  Successfully uninstalled transformers-4.38.2\n",
      "Found existing installation: accelerate 0.28.0\n",
      "Uninstalling accelerate-0.28.0:\n",
      "  Successfully uninstalled accelerate-0.28.0\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "Installing collected packages: accelerate, transformers\n",
      "Successfully installed accelerate-0.28.0 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate \n",
    "# Accelerate is a library that provides utilities for distributed and hardware-accelerated training of machine learning models, including transformer-based models.\n",
    "!pip uninstall -y transformers accelerate\n",
    " # The Transformers library provides a high-level API for working with pre-trained transformer models, such as BERT, GPT, and T5, in various NLP tasks.\n",
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.50.0-cp311-cp311-win_amd64.whl.metadata (162 kB)\n",
      "     ---------------------------------------- 0.0/162.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 162.6/162.6 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\prashant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prashant\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp311-cp311-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.6/7.6 MB 33.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.4/7.6 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.1/7.6 MB 40.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.6 MB 38.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 37.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl (187 kB)\n",
      "   ---------------------------------------- 0.0/187.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 187.6/187.6 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 34.5 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.2/103.2 kB ? eta 0:00:00\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.50.0 kiwisolver-1.4.5 matplotlib-3.8.3 pyparsing-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"google/pegasus-cnn_dailymail\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_pegasus= AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1cb0a88a1d4ca1991400b0c15608c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5df22415e6d4ccab79bc1c3d749619e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/347k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8d1c0abb524399aaf37ce53d91aa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/335k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5f3b1f2e0942f2b795cde3dfb4218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3762ed668b64b069d99beee02dd6308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affa59eebe0645a38335076a56dbeb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=load_dataset('samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"summary\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am trying to make summary based on inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_lengths=[len(dataset[split])for split in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 14732\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 819\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 818\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for split in dataset:\n",
    "    print(dataset[split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14732\n",
      "819\n",
      "818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for split in dataset:\n",
    "    print(len(dataset[split]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'dialogue', 'summary']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][\"dialogue\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][\"summary\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data_in_batch):\n",
    "    input_encoding=tokenizer(data_in_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
    "\n",
    "    target_encoding=tokenizer(data_in_batch[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encoding['input_ids'],\n",
    "        'attention_mask':  input_encoding['attention_mask'],\n",
    "        'labels': target_encoding['input_ids'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f67bb3a2904585a31fe0c3c1f3b88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d8ae93bdf2490997dde92bba909007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec13198dc9a4c14bc64761ea411077d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13818513',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
       " 'input_ids': [12195,\n",
       "  151,\n",
       "  125,\n",
       "  7091,\n",
       "  3659,\n",
       "  107,\n",
       "  842,\n",
       "  119,\n",
       "  245,\n",
       "  181,\n",
       "  152,\n",
       "  10508,\n",
       "  151,\n",
       "  7435,\n",
       "  147,\n",
       "  12195,\n",
       "  151,\n",
       "  125,\n",
       "  131,\n",
       "  267,\n",
       "  650,\n",
       "  119,\n",
       "  3469,\n",
       "  29344,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1]}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"pegasus-samsum\",\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=1e6, \n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_pegasus, \n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collator, \n",
    "                  train_dataset=dataset_samsum_pt[\"test\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a74d727601b4cff8830e3c0f6c5ba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8149, 'grad_norm': 5.16481876373291, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 1.8901, 'grad_norm': 10.615921020507812, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n",
      "{'loss': 1.9389, 'grad_norm': 4.53582239151001, 'learning_rate': 3e-06, 'epoch': 0.59}\n",
      "{'loss': 1.8459, 'grad_norm': 6.932342529296875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.78}\n",
      "{'loss': 1.8472, 'grad_norm': 5.430268287658691, 'learning_rate': 5e-06, 'epoch': 0.98}\n",
      "{'train_runtime': 5394.3965, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.009, 'train_loss': 1.8649560072842766, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=1.8649560072842766, metrics={'train_runtime': 5394.3965, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.009, 'train_loss': 1.8649560072842766, 'epoch': 1.0})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('samsum-tokenizer\\\\tokenizer_config.json',\n",
       " 'samsum-tokenizer\\\\special_tokens_map.json',\n",
       " 'samsum-tokenizer\\\\spiece.model',\n",
       " 'samsum-tokenizer\\\\added_tokens.json',\n",
       " 'samsum-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"samsum-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"C:\\prashant\\Workspace-TextSummarizer\\samsum-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= AutoModelForSeq2SeqLM.from_pretrained(\"C:\\prashant\\Workspace-TextSummarizer\\pegasus-samsum-model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_for_test=dataset[\"train\"][\"dialogue\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has to be celebrated.\n",
      "Demi: I agree! :D\n",
      "Demi: Tonight at Death & Co.?\n",
      "Lucas: Sure!\n",
      "Lucas: See you there at 10pm?\n",
      "Demi: Yeah! See you there! :D\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=dataset[\"train\"][\"summary\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs={\"length_penalty\":0.8,\"num_beams\":8, \"max_length\":128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_summarization_object=pipeline(\"summarization\", model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "predicted_text=pipeline_summarization_object(sample_text_for_test,**gen_kwargs)[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demi is promoted at Death & Co. tonight at 10pm. Demi will see Lucas there at 10pm. See you there there at Death & Co.\n"
     ]
    }
   ],
   "source": [
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant\\AppData\\Local\\Temp\\ipykernel_33004\\2696170338.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n",
      "c:\\Users\\Prashant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2dde364ca94c3ea9de6359f876c37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:08<00:00, 61.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>0.022888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1  rouge2    rougeL  rougeLsum\n",
       "pegasus  0.022886     0.0  0.022541   0.022888"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
